{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import math\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,r2_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Flatten, Bidirectional, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "forex = pd.read_csv('prep_forex.csv', header=[0,1], index_col=0)\n",
    "index = pd.read_csv('prep_index.csv', header=[0,1,2], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_features = {\"BDT\": [None, \"VND\"], \n",
    "               \"MNT\": [None, \"LKR\"],\n",
    "               ('PKR', 'Karachi 100'): [None, \"INR\", ('JPY', 'NIkkei 225')],\n",
    "               ('LKR', 'CSE All-Share'): [None, \"IDR\", ('MNT', 'MNE Top 20')]}\n",
    "\n",
    "feats = [('PKR', 'Karachi 100'), ('LKR', 'CSE All-Share'), \"BDT\", \"MNT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "forex_pairs = list(set([x[1] for x in forex.columns if x[0] == 'Close']))\n",
    "index_pairs = list(set([(x[1], x[2]) for x in index.columns if x[0] == 'Close']))\n",
    "\n",
    "scalers = [None, MinMaxScaler, MaxAbsScaler, StandardScaler, RobustScaler, Normalizer,\n",
    "           QuantileTransformer, PowerTransformer, FunctionTransformer]\n",
    "\n",
    "metric = 'Close'\n",
    "metrics = ['Open', 'Close', 'Low', 'High', 'Volume']\n",
    "# target = [metric]\n",
    "features = ['Intraday_OC', 'Prev_close_open'] + [y+x for x in ['', '_Ret', '_MTD', '_YTD'] for y in metrics]# if (x+y) not in target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(y_true, y_pred, model):\n",
    "    plot_df = pd.concat([pd.DataFrame(y_true), pd.DataFrame(y_pred)], axis=1, ignore_index=True)\n",
    "    plt.figure()\n",
    "    plt.plot(plot_df)\n",
    "    plt.title(\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_scale(X, y, scaler):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
    "    if(scaler):\n",
    "        scaler_X = scaler()\n",
    "        if(scaler == scalers[-1]):\n",
    "            scaler_X = scaler(np.log1p)\n",
    "        scaler_X = scaler_X.fit(X_train)\n",
    "        X_train = scaler_X.transform(X_train)\n",
    "        X_test = scaler_X.transform(X_test)\n",
    "    return(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bins(real, pred):\n",
    "    try:\n",
    "        y_test = Binarizer().transform(pd.DataFrame(real).pct_change().dropna())\n",
    "        y_pred = Binarizer().transform(pd.DataFrame(pred).pct_change().dropna())\n",
    "    except:\n",
    "        y_test = Binarizer().transform(pd.DataFrame(real))\n",
    "        y_pred = Binarizer().transform(pd.DataFrame(pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    return({\"F1\" :f1_score(y_test, y_pred, average='weighted'),\n",
    "        \"Precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "        \"Recall\": recall_score(y_test, y_pred, average='weighted')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(real, pred, extra_feat, is_exog):\n",
    "    save_data = pd.DataFrame(columns = ['y_true_class', 'y_pred_class', 'y_true_reg', 'y_pred_reg'], index = real.index)\n",
    "    save_data['y_true_reg'] = real.values\n",
    "    save_data['y_pred_reg'] = pred\n",
    "    if(metric[-3:] == 'Ret'):\n",
    "        save_data['y_true_class'] = np.sign(save_data['y_true_reg'])\n",
    "        save_data['y_pred_class'] = np.sign(save_data['y_pred_reg'])\n",
    "    else:\n",
    "        save_data['y_true_class'] = np.sign(save_data['y_true_reg'].pct_change())\n",
    "        save_data['y_pred_class'] = np.sign(save_data['y_pred_reg'].pct_change())\n",
    "    try:\n",
    "        save_data.to_csv(real.columns[0][1]+\"_\"+real.columns[0][0]+\"_\"+str(extra_feat)+\"_\"+str(is_exog)+\"_LSTM.csv\")\n",
    "    except:\n",
    "        save_data.to_csv(real.columns[0][1]+\"_\"+real.columns[0][0]+\"_\"+str(is_exog)+\"_LSTM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cross_domain_features(feat):\n",
    "    if(isinstance(feat, tuple)):\n",
    "        index_cols = index_cols = [x for x in index.columns if x[1] == feat[0] and x[2] == feat[1]]\n",
    "        X = index[[col for col in index_cols if col[0] in features + ['Time features']]][:-1]\n",
    "    else:\n",
    "        forex_cols = [x for x in forex.columns if x[1] == feat]\n",
    "        X = forex[[col for col in forex_cols if col[0] in features + ['Time features']]][:-1]\n",
    "    return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_forex_lstm(cur, target, extra_feat, is_exog=False, transf = None):\n",
    "    forex_cols = [x for x in forex.columns if x[1] == cur]\n",
    "    X = forex[[col for col in forex_cols if col[0] in features + ['Time features']]][:-1]\n",
    "    y = forex[[col for col in forex_cols if col[0] == metric]].shift(-1)[:-1]\n",
    "    if(extra_feat):\n",
    "            X = X.join(add_cross_domain_features(extra_feat))\n",
    "    X = X.dropna(how='all', axis=1)\n",
    "    X = X.dropna(how='any')\n",
    "    y = y[y.index.isin(X.index)]\n",
    "    X_train, X_test, y_train, y_test = split_scale(X, y, transf)\n",
    "    res, y_pred = run_lstm_model((X_train, y_train), (X_test, y_test), is_exog)\n",
    "    save_to_csv(y_test, y_pred, extra_feat, is_exog)\n",
    "    metrics = check_bins(y_test, y_pred)\n",
    "    res.update(metrics)\n",
    "    return(res)\n",
    "\n",
    "def do_index_lstm(cur, target, extra_feat, is_exog=False, transf = None):\n",
    "    index_cols = [x for x in index.columns if x[1] == cur[0] and x[2] == cur[1]]\n",
    "    X = index[[col for col in index_cols if col[0] in features + ['Time features']]][:-1]\n",
    "    y = index[[col for col in index_cols if col[0] == metric]].shift(-1)[:-1]\n",
    "    if(extra_feat):\n",
    "            X = X.join(add_cross_domain_features(extra_feat))\n",
    "    X = X.dropna(how='all', axis=1)\n",
    "    X = X.dropna(how='any')\n",
    "    y = y[y.index.isin(X.index)]\n",
    "    print(X)\n",
    "    print(y)\n",
    "    X_train, X_test, y_train, y_test = split_scale(X, y, transf)\n",
    "    res, y_pred = run_lstm_model((X_train, y_train), (X_test, y_test), is_exog)\n",
    "    save_to_csv(y_test, y_pred, extra_feat, is_exog)\n",
    "    metrics = check_bins(y_test, y_pred)\n",
    "    return(res.update(metrics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm_model(train, test, is_exog=False):\n",
    "    train_X, train_y = train\n",
    "    test_X, test_y = test\n",
    "    if(not is_exog):\n",
    "        X_train = None\n",
    "    train_X = train_X.values.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = test_X.values.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True))\n",
    "#     model.add(LSTM(100))\n",
    "    model.add(Dense(100))\n",
    "#     model.add(Dense(50))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "#     model.add(Activation('softmax'))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "    # plot history\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    yhat = model.predict(test_X)\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "    # invert scaling for forecast\n",
    "    yhat = yhat[:,0]\n",
    "    # invert scaling for actual\n",
    "    test_y = test_y.values.reshape((len(test_y), 1))\n",
    "    test_y = test_y[:,0]\n",
    "    # calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(test_y, yhat))\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    print('Test R2: %.3f' % r2_score(test_y, yhat))\n",
    "    plot_results(test_y, yhat, model)\n",
    "    return({}, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Close        High         Low        Open       Volume  \\\n",
      "Currency           PKR         PKR         PKR         PKR          PKR   \n",
      "Idx        Karachi 100 Karachi 100 Karachi 100 Karachi 100  Karachi 100   \n",
      "Date                                                                      \n",
      "2013-11-06   23165.210   23218.720   22802.580    22802.58  132131000.0   \n",
      "2013-11-07   23220.210   23350.130   23113.890    23224.92  120205000.0   \n",
      "2013-11-08   23367.150   23392.110   23152.990    23208.79   98147000.0   \n",
      "2013-11-11   23381.080   23588.520   23364.480    23371.83  158192000.0   \n",
      "2013-11-12   23302.460   23530.760   23274.770    23385.03  129138000.0   \n",
      "2013-11-13   23287.060   23312.650   23146.830    23276.92   91585000.0   \n",
      "2013-11-14   23351.896   23385.182   23179.998    23284.07  100347001.6   \n",
      "2013-11-15   23416.732   23457.714   23213.166    23291.22  109109003.2   \n",
      "2013-11-18   23611.240   23675.310   23312.670    23312.67  135395008.0   \n",
      "2013-11-19   23819.430   23880.080   23628.070    23628.07  173190000.0   \n",
      "2013-11-20   23800.970   24007.120   23775.770    23857.42  191206000.0   \n",
      "2013-11-21   23784.180   23914.560   23732.100    23813.63  106148000.0   \n",
      "2013-11-22   23847.570   23874.820   23702.850    23809.55   97088000.0   \n",
      "2013-11-25   23978.290   24089.850   23906.730    23908.48  112755000.0   \n",
      "2013-11-26   23798.700   23997.360   23745.730    23997.36   93193000.0   \n",
      "2013-11-27   23768.350   23849.010   23683.530    23784.98   74448000.0   \n",
      "2013-11-28   24180.490   24204.480   23817.880    23817.88  120788000.0   \n",
      "2013-11-29   24302.190   24338.890   24178.070    24195.75  111293000.0   \n",
      "2013-12-02   24401.570   24461.790   24318.160    24337.76  119086000.0   \n",
      "2013-12-03   24588.470   24618.690   24453.620    24453.62  138604000.0   \n",
      "2013-12-04   24445.380   24750.130   24427.890    24616.44  147564000.0   \n",
      "2013-12-05   24800.690   24866.040   24447.130    24466.00  140980992.0   \n",
      "2013-12-06   24870.550   25007.660   24808.830    24855.34  194684000.0   \n",
      "2013-12-09   24998.890   25125.810   24875.260    24875.26  103613000.0   \n",
      "2013-12-10   24878.680   25102.140   24852.470    25070.49  108316000.0   \n",
      "2013-12-11   24972.900   25020.170   24912.210    24912.21  116565000.0   \n",
      "2013-12-12   25072.000   25103.410   24917.520    25033.99  135908000.0   \n",
      "2013-12-13   25256.690   25285.620   25088.490    25088.49  220244000.0   \n",
      "2013-12-16   25298.320   25407.480   25262.650    25290.85  189106000.0   \n",
      "2013-12-17   25350.200   25399.200   25193.670    25313.56  194494000.0   \n",
      "...                ...         ...         ...         ...          ...   \n",
      "2019-06-20   34995.910   35061.540   34645.960    34691.80   92007000.0   \n",
      "2019-06-21   35125.250   35207.590   35036.620    35106.58   81667000.0   \n",
      "2019-06-24   34471.950   35194.640   34368.460    35147.18   58098000.0   \n",
      "2019-06-25   34190.620   34450.860   33984.600    34416.62  106959000.0   \n",
      "2019-06-26   34088.560   34290.580   33731.940    34261.32  115483000.0   \n",
      "2019-06-27   33774.420   34261.920   33521.380    34101.39  111732000.0   \n",
      "2019-06-28   33901.580   33936.230   33409.500    33776.38  149240000.0   \n",
      "2019-07-01   33996.330   34068.580   33825.850    33963.89   37683000.0   \n",
      "2019-07-02   34307.110   34340.230   33918.970    34013.28   73911000.0   \n",
      "2019-07-03   34896.550   34938.440   34320.930    34320.93   99351000.0   \n",
      "2019-07-04   34570.620   35279.320   34466.620    35132.54   88946000.0   \n",
      "2019-07-05   34190.020   34536.700   34062.980    34525.67   42353000.0   \n",
      "2019-07-08   33742.680   34229.630   33686.200    34184.57   46790000.0   \n",
      "2019-07-09   33855.580   34051.010   33656.210    33750.85   48351000.0   \n",
      "2019-07-10   33840.050   33955.000   33796.910    33887.27   25847000.0   \n",
      "2019-07-11   33875.400   33957.790   33724.980    33879.14   23394000.0   \n",
      "2019-07-12   33672.490   34001.760   33599.480    33865.68   43370000.0   \n",
      "2019-07-15   32958.350   33646.590   32895.820    33644.53   52230000.0   \n",
      "2019-07-16   32972.020   33093.790   32604.640    32914.92  119497000.0   \n",
      "2019-07-17   32981.990   33381.320   32835.050    33028.93   93841000.0   \n",
      "2019-07-18   32309.540   33014.740   32223.540    32949.25   71672000.0   \n",
      "2019-07-19   32458.770   32549.310   31901.280    32251.51  106386000.0   \n",
      "2019-07-22   32584.550   32723.680   32391.550    32625.27   38194000.0   \n",
      "2019-07-23   32715.880   32758.440   32455.100    32602.00   79622000.0   \n",
      "2019-07-24   32401.400   32765.080   32348.910    32726.15   72693000.0   \n",
      "2019-07-25   32446.400   32502.220   32150.260    32484.35   54379512.0   \n",
      "2019-07-26   32103.270   32439.160   32031.980    32438.51   67300000.0   \n",
      "2019-07-29   31734.230   32122.090   31717.760    32103.27   37374980.0   \n",
      "2019-07-30   31658.120   31774.210   31485.760    31734.23   40951512.0   \n",
      "2019-07-31   31938.480   31968.780   31545.260    31658.12          0.0   \n",
      "\n",
      "           Intraday_OC Prev_close_open    Open_Ret    High_Ret     Low_Ret  \\\n",
      "Currency           PKR             PKR         PKR         PKR         PKR   \n",
      "Idx        Karachi 100     Karachi 100 Karachi 100 Karachi 100 Karachi 100   \n",
      "Date                                                                         \n",
      "2013-11-06    0.015903        0.000521    0.017419    0.016852    0.017440   \n",
      "2013-11-07   -0.000203        0.002578    0.018522    0.005660    0.013652   \n",
      "2013-11-08    0.006823       -0.000492   -0.000695    0.001798    0.001692   \n",
      "2013-11-11    0.000396        0.000200    0.007025    0.008396    0.009134   \n",
      "2013-11-12   -0.003531        0.000169    0.000565   -0.002449   -0.003840   \n",
      "2013-11-13    0.000436       -0.001096   -0.004623   -0.009269   -0.005497   \n",
      "2013-11-14    0.002913       -0.000128    0.000307    0.003111    0.001433   \n",
      "2013-11-15    0.005389       -0.002598    0.000307    0.003102    0.001431   \n",
      "2013-11-18    0.012807       -0.004444    0.000921    0.009276    0.004287   \n",
      "2013-11-19    0.008099        0.000713    0.013529    0.008649    0.013529   \n",
      "2013-11-20   -0.002366        0.001595    0.009707    0.005320    0.006251   \n",
      "2013-11-21   -0.001237        0.000532   -0.001835   -0.003856   -0.001837   \n",
      "2013-11-22    0.001597        0.001067   -0.000171   -0.001662   -0.001233   \n",
      "2013-11-25    0.002920        0.002554    0.004155    0.009007    0.008601   \n",
      "2013-11-26   -0.008278        0.000795    0.003718   -0.003839   -0.006735   \n",
      "2013-11-27   -0.000699       -0.000577   -0.008850   -0.006182   -0.002619   \n",
      "2013-11-28    0.015224        0.002084    0.001383    0.014905    0.005673   \n",
      "2013-11-29    0.004399        0.000631    0.015865    0.005553    0.015123   \n",
      "2013-12-02    0.002622        0.001464    0.005869    0.005050    0.005794   \n",
      "2013-12-03    0.005515        0.002133    0.004761    0.006414    0.005570   \n",
      "2013-12-04   -0.006949        0.001138    0.006658    0.005339   -0.001052   \n",
      "2013-12-05    0.013680        0.000844   -0.006111    0.004683    0.000788   \n",
      "2013-12-06    0.000612        0.002204    0.015914    0.005695    0.014795   \n",
      "2013-12-09    0.004970        0.000189    0.000801    0.004725    0.002678   \n",
      "2013-12-10   -0.007651        0.002864    0.007848   -0.000942   -0.000916   \n",
      "2013-12-11    0.002436        0.001348   -0.006313   -0.003265    0.002404   \n",
      "2013-12-12    0.001518        0.002446    0.004888    0.003327    0.000213   \n",
      "2013-12-13    0.006704        0.000658    0.002177    0.007258    0.006861   \n",
      "2013-12-16    0.000295        0.001353    0.008066    0.004819    0.006942   \n",
      "2013-12-17    0.001447        0.000602    0.000898   -0.000326   -0.002731   \n",
      "...                ...             ...         ...         ...         ...   \n",
      "2019-06-20    0.008766        0.001030    0.000527    0.007113    0.003579   \n",
      "2019-06-21    0.000532        0.003162    0.011956    0.004166    0.011276   \n",
      "2019-06-24   -0.019211        0.000624    0.001156   -0.000368   -0.019070   \n",
      "2019-06-25   -0.006567       -0.001605   -0.020786   -0.021133   -0.011169   \n",
      "2019-06-26   -0.005042        0.002068   -0.004512   -0.004652   -0.007435   \n",
      "2019-06-27   -0.009588        0.000376   -0.004668   -0.000836   -0.006242   \n",
      "2019-06-28    0.003707        0.000058   -0.009531   -0.009506   -0.003338   \n",
      "2019-07-01    0.000955        0.001838    0.005552    0.003900    0.012462   \n",
      "2019-07-02    0.008639        0.000499    0.001454    0.007974    0.002753   \n",
      "2019-07-03    0.016772        0.000403    0.009045    0.017420    0.011851   \n",
      "2019-07-04   -0.015994        0.006763    0.023648    0.009757    0.004245   \n",
      "2019-07-05   -0.009722       -0.001300   -0.017274   -0.021050   -0.011711   \n",
      "2019-07-08   -0.012927       -0.000159   -0.009880   -0.008891   -0.011061   \n",
      "2019-07-09    0.003103        0.000242   -0.012688   -0.005218   -0.000890   \n",
      "2019-07-10   -0.001393        0.000936    0.004042   -0.002820    0.004181   \n",
      "2019-07-11   -0.000110        0.001155   -0.000240    0.000082   -0.002128   \n",
      "2019-07-12   -0.005705       -0.000287   -0.000397    0.001295   -0.003721   \n",
      "2019-07-15   -0.020395       -0.000830   -0.006530   -0.010446   -0.020943   \n",
      "2019-07-16    0.001735       -0.001318   -0.021686   -0.016430   -0.008852   \n",
      "2019-07-17   -0.001421        0.001726    0.003464    0.008688    0.007067   \n",
      "2019-07-18   -0.019415       -0.000993   -0.002412   -0.010982   -0.018624   \n",
      "2019-07-19    0.006426       -0.001796   -0.021176   -0.014098   -0.010001   \n",
      "2019-07-22   -0.001248        0.005130    0.011589    0.005357    0.015368   \n",
      "2019-07-23    0.003493        0.000536   -0.000713    0.001062    0.001962   \n",
      "2019-07-24   -0.009923        0.000314    0.003808    0.000203   -0.003272   \n",
      "2019-07-25   -0.001168        0.002560   -0.007389   -0.008023   -0.006141   \n",
      "2019-07-26   -0.010335       -0.000243   -0.001411   -0.001940   -0.003679   \n",
      "2019-07-29   -0.011495        0.000000   -0.010335   -0.009774   -0.009810   \n",
      "2019-07-30   -0.002398        0.000000   -0.011495   -0.010830   -0.007315   \n",
      "2019-07-31    0.008856        0.000000   -0.002398    0.006124    0.001890   \n",
      "\n",
      "             Close_Ret  Volume_Ret    Open_MTD    High_MTD     Low_MTD  \\\n",
      "Currency           PKR         PKR         PKR         PKR         PKR   \n",
      "Idx        Karachi 100 Karachi 100 Karachi 100 Karachi 100 Karachi 100   \n",
      "Date                                                                     \n",
      "2013-11-06    0.016433         inf    0.001224    0.008919    0.008836   \n",
      "2013-11-07    0.002374   -0.090259    0.019768    0.014629    0.022609   \n",
      "2013-11-08    0.006328   -0.183503    0.019060    0.016453    0.024339   \n",
      "2013-11-11    0.000596    0.611786    0.026219    0.024988    0.033695   \n",
      "2013-11-12   -0.003363   -0.183663    0.026798    0.022478    0.029726   \n",
      "2013-11-13   -0.000661   -0.290797    0.022051    0.013001    0.024066   \n",
      "2013-11-14    0.002784    0.095671    0.022365    0.016152    0.025533   \n",
      "2013-11-15    0.002776    0.087317    0.022679    0.019304    0.027001   \n",
      "2013-11-18    0.008306    0.240915    0.023621    0.028759    0.031403   \n",
      "2013-11-19    0.008817    0.279146    0.037470    0.037657    0.045357   \n",
      "2013-11-20   -0.000775    0.104024    0.047540    0.043177    0.051892   \n",
      "2013-11-21   -0.000705   -0.444850    0.045617    0.039155    0.049960   \n",
      "2013-11-22    0.002665   -0.085353    0.045438    0.037429    0.048666   \n",
      "2013-11-25    0.005481    0.161369    0.049782    0.046772    0.057686   \n",
      "2013-11-26   -0.007490   -0.173491    0.053685    0.042753    0.050563   \n",
      "2013-11-27   -0.001275   -0.201142    0.044359    0.036307    0.047811   \n",
      "2013-11-28    0.017340    0.622448    0.045804    0.051753    0.053755   \n",
      "2013-11-29    0.005033   -0.078609    0.062396    0.057594    0.069690   \n",
      "2013-12-02    0.004089    0.070022    0.000000    0.000000    0.000000   \n",
      "2013-12-03    0.007659    0.163898    0.004761    0.006414    0.005570   \n",
      "2013-12-04   -0.005819    0.064645    0.011451    0.011787    0.004512   \n",
      "2013-12-05    0.014535   -0.044611    0.005269    0.016526    0.005303   \n",
      "2013-12-06    0.002817    0.380924    0.021267    0.022315    0.020177   \n",
      "2013-12-09    0.005160   -0.467789    0.022085    0.027145    0.022909   \n",
      "2013-12-10   -0.004809    0.045390    0.030107    0.026178    0.021972   \n",
      "2013-12-11    0.003787    0.076157    0.023603    0.022827    0.024428   \n",
      "2013-12-12    0.003968    0.165942    0.028607    0.026229    0.024647   \n",
      "2013-12-13    0.007366    0.620537    0.030846    0.033678    0.031677   \n",
      "2013-12-16    0.001648   -0.141380    0.039161    0.038660    0.038839   \n",
      "2013-12-17    0.002051    0.028492    0.040094    0.038321    0.036002   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "2019-06-20    0.009805    0.240003   -0.033398   -0.023096   -0.022000   \n",
      "2019-06-21    0.003696   -0.112383   -0.021841   -0.019027   -0.010972   \n",
      "2019-06-24   -0.018599   -0.288599   -0.020710   -0.019388   -0.029833   \n",
      "2019-06-25   -0.008161    0.841010   -0.041065   -0.040111   -0.040669   \n",
      "2019-06-26   -0.002985    0.079694   -0.045393   -0.044577   -0.047801   \n",
      "2019-06-27   -0.009215   -0.032481   -0.049849   -0.045376   -0.053745   \n",
      "2019-06-28    0.003765    0.335696   -0.058904   -0.054450   -0.056903   \n",
      "2019-07-01    0.002795   -0.747501    0.000000    0.000000    0.000000   \n",
      "2019-07-02    0.009142    0.961388    0.001454    0.007974    0.002753   \n",
      "2019-07-03    0.017181    0.344198    0.010512    0.025533    0.014636   \n",
      "2019-07-04   -0.009340   -0.104730    0.034409    0.035538    0.018943   \n",
      "2019-07-05   -0.011009   -0.523835    0.016541    0.013741    0.007010   \n",
      "2019-07-08   -0.013084    0.104762    0.006497    0.004727   -0.004128   \n",
      "2019-07-09    0.003346    0.033362   -0.006273   -0.000516   -0.005015   \n",
      "2019-07-10   -0.000459   -0.465430   -0.002256   -0.003334   -0.000856   \n",
      "2019-07-11    0.001045   -0.094905   -0.002495   -0.003252   -0.002982   \n",
      "2019-07-12   -0.005990    0.853894   -0.002892   -0.001961   -0.006692   \n",
      "2019-07-15   -0.021208    0.204289   -0.009403   -0.012386   -0.027495   \n",
      "2019-07-16    0.000415    1.287900   -0.030885   -0.028613   -0.036103   \n",
      "2019-07-17    0.000302   -0.214700   -0.027528   -0.020173   -0.029291   \n",
      "2019-07-18   -0.020388   -0.236240   -0.029874   -0.030933   -0.047369   \n",
      "2019-07-19    0.004619    0.484345   -0.050418   -0.044594   -0.056896   \n",
      "2019-07-22    0.003875   -0.640987   -0.039413   -0.039476   -0.042402   \n",
      "2019-07-23    0.004030    1.084673   -0.040098   -0.038456   -0.040524   \n",
      "2019-07-24   -0.009612   -0.087024   -0.036443   -0.038261   -0.043663   \n",
      "2019-07-25    0.001389   -0.251929   -0.043562   -0.045977   -0.049536   \n",
      "2019-07-26   -0.010575    0.237598   -0.044912   -0.047828   -0.053033   \n",
      "2019-07-29   -0.011495   -0.444651   -0.054782   -0.057134   -0.062322   \n",
      "2019-07-30   -0.002398    0.095693   -0.065648   -0.067346   -0.069181   \n",
      "2019-07-31    0.008856   -1.000000   -0.067889   -0.061635   -0.067422   \n",
      "\n",
      "             Close_MTD    Open_YTD    High_YTD     Low_YTD   Close_YTD  \n",
      "Currency           PKR         PKR         PKR         PKR         PKR  \n",
      "Idx        Karachi 100 Karachi 100 Karachi 100 Karachi 100 Karachi 100  \n",
      "Date                                                                    \n",
      "2013-11-06    0.022788    0.348443    0.371007    0.358308    0.379303  \n",
      "2013-11-07    0.025216    0.373419    0.378767    0.376852    0.382578  \n",
      "2013-11-08    0.031704    0.372465    0.381245    0.379181    0.391327  \n",
      "2013-11-11    0.032319    0.382106    0.392843    0.391779    0.392156  \n",
      "2013-11-12    0.028848    0.382887    0.389432    0.386435    0.387475  \n",
      "2013-11-13    0.028168    0.376494    0.376554    0.378814    0.386558  \n",
      "2013-11-14    0.031030    0.376916    0.380836    0.380790    0.390418  \n",
      "2013-11-15    0.033893    0.377339    0.385119    0.382766    0.394279  \n",
      "2013-11-18    0.042481    0.378608    0.397968    0.388693    0.405860  \n",
      "2013-11-19    0.051673    0.397259    0.410059    0.407481    0.418256  \n",
      "2013-11-20    0.050858    0.410822    0.417560    0.416279    0.417157  \n",
      "2013-11-21    0.050116    0.408232    0.412095    0.413678    0.416157  \n",
      "2013-11-22    0.052915    0.407991    0.409748    0.411935    0.419932  \n",
      "2013-11-25    0.058687    0.413841    0.422445    0.424080    0.427715  \n",
      "2013-11-26    0.050757    0.419097    0.416984    0.414490    0.417022  \n",
      "2013-11-27    0.049417    0.406538    0.408224    0.410784    0.415215  \n",
      "2013-11-28    0.067614    0.408484    0.429214    0.418787    0.439755  \n",
      "2013-11-29    0.072987    0.430829    0.437150    0.440243    0.447001  \n",
      "2013-12-02    0.000000    0.439227    0.444407    0.448588    0.452918  \n",
      "2013-12-03    0.007659    0.446078    0.453672    0.456657    0.464046  \n",
      "2013-12-04    0.001795    0.455707    0.461433    0.455125    0.455527  \n",
      "2013-12-05    0.016356    0.446811    0.468277    0.456271    0.476682  \n",
      "2013-12-06    0.019219    0.469834    0.476640    0.477817    0.480842  \n",
      "2013-12-09    0.024479    0.471012    0.483616    0.481774    0.488484  \n",
      "2013-12-10    0.019552    0.482557    0.482218    0.480416    0.481326  \n",
      "2013-12-11    0.023414    0.473197    0.477378    0.483975    0.486936  \n",
      "2013-12-12    0.027475    0.480399    0.482293    0.484291    0.492837  \n",
      "2013-12-13    0.035044    0.483622    0.493052    0.494475    0.503834  \n",
      "2013-12-16    0.036750    0.495588    0.500248    0.504850    0.506312  \n",
      "2013-12-17    0.038876    0.496931    0.499759    0.500741    0.509401  \n",
      "...                ...         ...         ...         ...         ...  \n",
      "2019-06-20   -0.014347   -0.064103   -0.078467   -0.064791   -0.078952  \n",
      "2019-06-21   -0.010704   -0.052914   -0.074629   -0.054245   -0.075548  \n",
      "2019-06-24   -0.029104   -0.051818   -0.074969   -0.072281   -0.092742  \n",
      "2019-06-25   -0.037027   -0.071527   -0.094518   -0.082643   -0.100146  \n",
      "2019-06-26   -0.039902   -0.075717   -0.098731   -0.089463   -0.102833  \n",
      "2019-06-27   -0.048750   -0.080031   -0.099484   -0.095147   -0.111100  \n",
      "2019-06-28   -0.045168   -0.088799   -0.108044   -0.098167   -0.107754  \n",
      "2019-07-01    0.000000   -0.083740   -0.104565   -0.086928   -0.105260  \n",
      "2019-07-02    0.009142   -0.082408   -0.097426   -0.084415   -0.097081  \n",
      "2019-07-03    0.026480   -0.074108   -0.081703   -0.073564   -0.081567  \n",
      "2019-07-04    0.016893   -0.052213   -0.072743   -0.069632   -0.090145  \n",
      "2019-07-05    0.005697   -0.068585   -0.092262   -0.080527   -0.100162  \n",
      "2019-07-08   -0.007461   -0.077787   -0.100333   -0.090698   -0.111936  \n",
      "2019-07-09   -0.004140   -0.089488   -0.105027   -0.091507   -0.108964  \n",
      "2019-07-10   -0.004597   -0.085807   -0.107551   -0.087709   -0.109373  \n",
      "2019-07-11   -0.003557   -0.086027   -0.107477   -0.089651   -0.108443  \n",
      "2019-07-12   -0.009526   -0.086390   -0.106322   -0.093039   -0.113783  \n",
      "2019-07-15   -0.030532   -0.092356   -0.115657   -0.112033   -0.132578  \n",
      "2019-07-16   -0.030130   -0.112039   -0.130186   -0.119893   -0.132218  \n",
      "2019-07-17   -0.029837   -0.108963   -0.122629   -0.113673   -0.131956  \n",
      "2019-07-18   -0.049617   -0.111113   -0.132264   -0.130180   -0.149654  \n",
      "2019-07-19   -0.045227   -0.129936   -0.144497   -0.138879   -0.145727  \n",
      "2019-07-22   -0.041527   -0.119853   -0.139914   -0.125645   -0.142416  \n",
      "2019-07-23   -0.037664   -0.120481   -0.139000   -0.123929   -0.138960  \n",
      "2019-07-24   -0.046915   -0.117132   -0.138826   -0.126796   -0.147236  \n",
      "2019-07-25   -0.045591   -0.123655   -0.145735   -0.132158   -0.146052  \n",
      "2019-07-26   -0.055684   -0.124891   -0.147392   -0.135351   -0.155083  \n",
      "2019-07-29   -0.066540   -0.133935   -0.155726   -0.143833   -0.164795  \n",
      "2019-07-30   -0.068778   -0.143891   -0.164869   -0.150095   -0.166799  \n",
      "2019-07-31   -0.060532   -0.145944   -0.159755   -0.148489   -0.159420  \n",
      "\n",
      "[1496 rows x 20 columns]\n",
      "                 Close\n",
      "Currency           PKR\n",
      "Idx        Karachi 100\n",
      "Date                  \n",
      "2013-11-06   23220.210\n",
      "2013-11-07   23367.150\n",
      "2013-11-08   23381.080\n",
      "2013-11-11   23302.460\n",
      "2013-11-12   23287.060\n",
      "2013-11-13   23351.896\n",
      "2013-11-14   23416.732\n",
      "2013-11-15   23611.240\n",
      "2013-11-18   23819.430\n",
      "2013-11-19   23800.970\n",
      "2013-11-20   23784.180\n",
      "2013-11-21   23847.570\n",
      "2013-11-22   23978.290\n",
      "2013-11-25   23798.700\n",
      "2013-11-26   23768.350\n",
      "2013-11-27   24180.490\n",
      "2013-11-28   24302.190\n",
      "2013-11-29   24401.570\n",
      "2013-12-02   24588.470\n",
      "2013-12-03   24445.380\n",
      "2013-12-04   24800.690\n",
      "2013-12-05   24870.550\n",
      "2013-12-06   24998.890\n",
      "2013-12-09   24878.680\n",
      "2013-12-10   24972.900\n",
      "2013-12-11   25072.000\n",
      "2013-12-12   25256.690\n",
      "2013-12-13   25298.320\n",
      "2013-12-16   25350.200\n",
      "2013-12-17   25524.460\n",
      "...                ...\n",
      "2019-06-20   35125.250\n",
      "2019-06-21   34471.950\n",
      "2019-06-24   34190.620\n",
      "2019-06-25   34088.560\n",
      "2019-06-26   33774.420\n",
      "2019-06-27   33901.580\n",
      "2019-06-28   33996.330\n",
      "2019-07-01   34307.110\n",
      "2019-07-02   34896.550\n",
      "2019-07-03   34570.620\n",
      "2019-07-04   34190.020\n",
      "2019-07-05   33742.680\n",
      "2019-07-08   33855.580\n",
      "2019-07-09   33840.050\n",
      "2019-07-10   33875.400\n",
      "2019-07-11   33672.490\n",
      "2019-07-12   32958.350\n",
      "2019-07-15   32972.020\n",
      "2019-07-16   32981.990\n",
      "2019-07-17   32309.540\n",
      "2019-07-18   32458.770\n",
      "2019-07-19   32584.550\n",
      "2019-07-22   32715.880\n",
      "2019-07-23   32401.400\n",
      "2019-07-24   32446.400\n",
      "2019-07-25   32103.270\n",
      "2019-07-26   31734.230\n",
      "2019-07-29   31658.120\n",
      "2019-07-30   31938.480\n",
      "2019-07-31   31839.110\n",
      "\n",
      "[1496 rows x 1 columns]\n",
      "(1196, 1, 20) (1196, 1) (300, 1, 20) (300, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1196 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: nan - val_loss: nan\n",
      "Epoch 2/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 3/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 4/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 5/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 6/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 7/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 8/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 9/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 10/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 11/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 12/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 13/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 14/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 15/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 16/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 17/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 18/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 19/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 20/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 21/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 22/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 23/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 24/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 25/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 26/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 27/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 28/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 29/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 30/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 31/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 32/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 33/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 34/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 35/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 36/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 37/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 38/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 39/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 40/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 41/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      " - 0s - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      " - 0s - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-030745c2633a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mextra_feat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcd_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcur\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mis_exog\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m             \u001b[0mdo_index_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_exog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_exog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-52-36f909ea2a46>\u001b[0m in \u001b[0;36mdo_index_lstm\u001b[1;34m(cur, target, extra_feat, is_exog, transf)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_scale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_lstm_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_exog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0msave_to_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_exog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-6f2a3ce979ce>\u001b[0m in \u001b[0;36mrun_lstm_model\u001b[1;34m(train, test, is_exog)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mtest_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# calculate RMSE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mrmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test RMSE: %.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mrmse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test R2: %.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \"\"\"\n\u001b[0;32m    251\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[1;32m--> 252\u001b[1;33m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[0;32m    253\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 578\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                     (type_err,\n\u001b[1;32m---> 60\u001b[1;33m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[0;32m     61\u001b[0m             )\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "for cur in feats:\n",
    "    for extra_feat in cd_features[cur]:\n",
    "        for is_exog in [True, False]:\n",
    "            do_index_lstm(cur, metric, extra_feat, is_exog=is_exog)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
